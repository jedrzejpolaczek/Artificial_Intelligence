{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Mathematically speaking the whole goal is to go from $E(W, b)$ to $E(W', b')$ that gives smaller value. We can achieve that by using the descent gradient method. That will allow us get better prediction $\\hat y = \\sigma(W'x + b')$. So from here, we can say... Gradient Descent function is a function of weights.\n",
    "\n",
    "OK, so a little story: we want to predict in which direction we should go, thats mean we want to calculate predicted value of error and choose the smaller one. And what is the best way to predict something in mathematics? Derivative! So to calculate change of value of error function we just need to calculate:\n",
    "\n",
    "$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "Here we can see why the sigmoid function is so useful. Counting derivative will be very easy because of exp. Here a little proof:\n",
    "\n",
    "$\\sigma'(z) = \\dots$\n",
    "\n",
    "$= \\frac{\\partial}{\\partial x}\\frac{1}{1 + e^{-z}} = \\dots$\n",
    "\n",
    "$= \\frac{e^{-z}}{(1 + e^{-z})^2} = \\dots$\n",
    "\n",
    "$= \\frac{1}{1 + e^{-z}}\\frac{e^{-z}}{1 + e^{-z}} = \\dots$\n",
    "\n",
    "$= \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "and now, let's recall that if we have mm points labelled $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$, the error formula is:\n",
    "\n",
    "* $E = -\\frac{1}{n}{\\sum_{i=1}^n(1 - y_i)(ln(1 -\\hat y_i)) - y_iln(\\hat y_i)}$ \n",
    "* where the prediction is given by $\\hat y_i^n = \\sigma(h_i)$\n",
    "* and $h_i$ we calculate from $h_i = W\\cdot x + b$\n",
    "\n",
    "Our goal is to calculate the gradient of $E$, at a point $x = (x_1, \\ldots, x_n)$, given by the partial derivatives\n",
    "\n",
    "$\\nabla E =\\left(\\frac{\\partial}{\\partial w_1}E, \\cdots, \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E \\right)$\n",
    "\n",
    "To simplify our calculations, we'll actually think of the error that each point produces, and calculate the derivative of this error. The total error, then, is the average of the errors at all the points. The error produced by each point is, simply:\n",
    "\n",
    "$E = - y \\ln(\\hat{y}) - (1-y) \\ln (1-\\hat{y})$\n",
    "\n",
    "In order to calculate the derivative of this error with respect to the weights, we'll first calculate $\\frac{\\partial}{\\partial w_j} \\hat{y}$. Recall that $\\hat{y} = \\sigma(Wx+b)$, so:\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_j} \\hat y = \\dots$\n",
    "\n",
    "$= \\frac{\\partial}{\\partial w_j}\\sigma(Wx + b) = \\dots$\n",
    "\n",
    "$= \\sigma(Wx + b)(1 - \\sigma(Wx +b))\\frac{\\partial}{\\partial w_j}(Wx + b) = \\dots$\n",
    "\n",
    "$= \\hat y(1 - \\hat y)\\frac{\\partial}{\\partial w_j}(Wx + b) = \\dots$\n",
    "\n",
    "$= \\hat y(1 - \\hat y)\\frac{\\partial}{\\partial w_j}(w_1x_1+ \\dots + w_jx_j + \\dots + w_nx_n + b) = \\dots$\n",
    "\n",
    "$= \\hat y(1 - \\hat y)x_j$\n",
    "\n",
    "So $\\frac{\\partial}{\\partial w_j} \\hat y = \\hat y(1 - \\hat y)x_j$\n",
    "\n",
    "\n",
    "The last equality is because the only term in the sum which is not a constant with respect to $w_j$ is precisely $w_j x_j$, which clearly has derivative $x_j$.\n",
    "\n",
    "Now, we can go ahead and calculate the derivative of the error $E$ at a point $x$, with respect to the weight $w_j$.\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_j} E = \\dots$\n",
    "\n",
    "$= \\frac{\\partial}{\\partial w_j}(-ylog(\\hat y) - (1 - y)log(1 - \\hat y)) = \\dots$\n",
    "\n",
    "$= -y\\frac{\\partial}{\\partial w_j}log(\\hat y) - (1 - y)\\frac{\\partial}{\\partial w_j}log(1 - \\hat y) = \\dots$\n",
    "\n",
    "$= -y \\frac{1}{\\hat y}\\hat y\\frac{\\partial}{\\partial w_j}\\hat y - (1 - y)\\frac{1}{1 - \\hat y}\\frac{\\partial}{\\partial w_j}(1 - \\hat y) = \\dots$\n",
    "\n",
    "$= -y \\frac{1}{\\hat y}\\hat y(1 - \\hat y)x_j - (1 - y)\\frac{1}{1 - \\hat y}(-1)\\hat y(1 - \\hat y)x_j = \\dots$\n",
    "\n",
    "$= -y(1 - \\hat y)x_j + (1 - y)\\hat yx_j = \\dots$\n",
    "\n",
    "$= -(y - \\hat y)x_j$\n",
    "\n",
    "So $\\frac{\\partial}{\\partial w_j} E = -(y - \\hat y)x_j$ and from simillar calculations $\\frac{\\partial}{\\partial b} E = -(y - \\hat y)$\n",
    "\n",
    "This actually tells us something very important. For a point with coordinates $(x_1, \\ldots, x_n)$, label $y$, and prediction $\\hat{y}$, the gradient of the error function at that point is $(-(y - \\hat{y})x_1, \\cdots, -(y - \\hat{y})x_n, -(y - \\hat{y}))$. In summary, the gradient is: \n",
    "\n",
    "$ \\nabla E = -(y - \\hat{y}) (x_1, \\ldots, x_n, 1)$.  \n",
    "\n",
    "The gradient is actually a scalar times the coordinates of the point! And what is the scalar? Nothing less than a multiple of the difference between the label and the prediction.\n",
    "\n",
    "#### Gradient Descent Step\n",
    "Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:\n",
    "\n",
    "$w_i' \\leftarrow w_i -\\alpha [-(y - \\hat{y}) x_i]$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$w_i' \\leftarrow w_i + \\alpha (y - \\hat{y}) x_i$\n",
    "\n",
    "Similarly, it updates the bias in the following way:\n",
    "\n",
    "$b' \\leftarrow b + \\alpha (y - \\hat{y})$\n",
    "\n",
    "*Note*: Since we've taken the average of the errors, the term we are adding should be $\\frac{1}{m} \\cdot \\alpha$ instead of $\\alpha$, but as $\\alpha$ is a constant, then in order to simplify calculations, we'll just take $\\frac{1}{m} \\cdot \\alpha$ to be our learning rate, and abuse the notation by just calling it $\\alpha$.\n",
    "\n",
    "#### Squared Errors (SSE)\n",
    "* SSE: $E = \\frac{1}{2}(y -\\hat y)^2$\n",
    "* Updating weight: $w_i = w_i + \\Delta w_i = \\eta(y - \\hat y)f'(h)x_i = w_i + \\eta \\delta x_i$\n",
    "* Gradient: $\\Delta w_i - \\alpha \\frac{\\partial E}{\\partial w_i}$\n",
    "* Learning rate: $\\eta$ in $\\Delta w_i = -\\eta \\frac{\\partial E}{\\partial w_i}$\n",
    "* Error term: $\\delta = (y - \\hat y)f'(h)$\n",
    "\n",
    "Example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consolidated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# DONE: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of implementation Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f128f6b7bdc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Split into features and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ix'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data preperation\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Activation of the output unit\n",
    "        #   Notice we multiply the inputs and the weights here \n",
    "        #   rather than storing h as a separate variable \n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # The error, the target minus the network output\n",
    "        error = y - output\n",
    "\n",
    "        # The error term\n",
    "        #   Notice we calulate f'(h) here instead of defining a separate\n",
    "        #   sigmoid_prime function. This just makes it faster because we\n",
    "        #   can re-use the result of the sigmoid function stored in\n",
    "        #   the output variable\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "        # The gradient descent step, the error times the gradient times the inputs\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # Update the weights here. The learning rate times the \n",
    "    # change in weights, divided by the number of records to average\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data]",
   "language": "python",
   "name": "conda-env-data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
